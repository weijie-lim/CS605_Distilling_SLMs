# CS605_Distilling_SLMs
Testing whether LMs can be distilled into smaller LMs and retain performance through Rationale Learning

## General Idea (References below)
<img width="628" alt="image" src="https://github.com/weijie-lim/CS605_Distilling_SLMs/assets/47061871/14ed264f-e3ca-4598-973a-c882b34aa4cd">


## Our process
In this repository, we attempt to take a semi-large LM in the form of FLAN T5 LARGE (11B parameters) and try to distil knowledge from it into a smaller model FLAN T5 SMALL (80M parameters). We follow the idea taken from the paper in the reference named "Distilling step by step!". 

## Results
Our results are shown in the slides uploaded in the repository. 

## Reference
Hsieh, C.-Y., Li, C.-L., Yeh, C.-K., Nakhost, H., Fujii, Y., Ratner, A., Krishna, R., Lee, C.-Y., &amp; Pfister, T. (2023, May 3). Distilling step-by-step! outperforming larger language models with less training data and smaller model sizes. arXiv.org. https://arxiv.org/abs/2305.02301  

## Credits to my Team mates
1. Quah Zong You
2. Keith Wong
3. Colin Jiang
4. Chia De Han
